{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\n\nfrom PIL import Image,ImageFile\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\n# import segmentation_models_pytorch as smp\n\nimport torch\nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n# device=\"cpu\"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:18.863072Z","iopub.execute_input":"2021-08-26T23:44:18.863741Z","iopub.status.idle":"2021-08-26T23:44:22.440940Z","shell.execute_reply.started":"2021-08-26T23:44:18.863628Z","shell.execute_reply":"2021-08-26T23:44:22.439846Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Utility Functions Provided by Kaggle**\n\n* Here is the utility function which is used to create mask from this **Run Length Encoding**(RLE) data.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n    component = np.zeros((height, width), np.float32)\n    component = component.reshape(-1)\n    rle = np.array([int(s) for s in rle.strip().split(' ')])\n    rle = rle.reshape(-1, 2)\n    start = 0\n    for index, length in rle:\n        start = start+index\n        end = start+length\n        component[start: end] = fill_value\n        start = end\n    component = component.reshape(width, height).T\n    return component","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.444934Z","iopub.execute_input":"2021-08-26T23:44:22.445231Z","iopub.status.idle":"2021-08-26T23:44:22.455007Z","shell.execute_reply.started":"2021-08-26T23:44:22.445202Z","shell.execute_reply":"2021-08-26T23:44:22.454073Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None):\n\n    if input_space == \"BGR\":\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() > 1 and input_range[1] == 1:\n            x = x / 255.0\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.458947Z","iopub.execute_input":"2021-08-26T23:44:22.459373Z","iopub.status.idle":"2021-08-26T23:44:22.469156Z","shell.execute_reply.started":"2021-08-26T23:44:22.459332Z","shell.execute_reply":"2021-08-26T23:44:22.468261Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SIIMDataset(Dataset):\n    \n    def __init__(self, df, data_dir, transform=None, preprocessing_fun=None, channel_first=True):\n        self.data_dir = data_dir\n        self.transform = transform                       # for augmentations\n        self.preprocessing_fun = preprocessing_fun       # preprocessing_fun to normalize images\n        self.channel_first = channel_first               # set channels as first dimension\n        self.image_ids = df.ImageId.values\n        self.group_by = df.groupby('ImageId')\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        df = self.group_by.get_group(img_id)\n        annotations = df[' EncodedPixels'].tolist()\n        \n        img_path = os.path.join(self.data_dir, img_id + \".png\")\n        img = Image.open(img_path).convert('RGB')\n        img = np.array(img)\n\n        mask = np.zeros(shape=(1024,1024))\n        if annotations[0] != ' -1':\n            for rle in annotations:\n                mask += run_length_decode(rle)\n        mask = (mask >= 1).astype('float32')\n        mask = np.expand_dims(mask, axis=-1)\n        \n        # apply augmentation\n        if self.transform:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n\n        if self.preprocessing_fun:\n            img = self.preprocessing_fun(img,\n                                         input_range=[0, 1],\n                                         mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        \n        # convert shape from (width, height, channel) ----> (channel, width, height) \n        if self.channel_first:\n            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n            mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)\n\n        return {\n            'image': torch.Tensor(img),\n            'mask': torch.Tensor(mask)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.470565Z","iopub.execute_input":"2021-08-26T23:44:22.470953Z","iopub.status.idle":"2021-08-26T23:44:22.487563Z","shell.execute_reply.started":"2021-08-26T23:44:22.470914Z","shell.execute_reply":"2021-08-26T23:44:22.486760Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    for data in tqdm(data_loader):\n        inputs = data['image']\n        labels = data['mask']\n\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    return train_loss/len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.493206Z","iopub.execute_input":"2021-08-26T23:44:22.493591Z","iopub.status.idle":"2021-08-26T23:44:22.502970Z","shell.execute_reply.started":"2021-08-26T23:44:22.493563Z","shell.execute_reply":"2021-08-26T23:44:22.501908Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def evaluate(data_loader, model, criterion):\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for data in tqdm(data_loader):\n            inputs = data['image']\n            labels = data['mask']\n\n            inputs = inputs.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.float)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n\n    return eval_loss/len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.505981Z","iopub.execute_input":"2021-08-26T23:44:22.506430Z","iopub.status.idle":"2021-08-26T23:44:22.516134Z","shell.execute_reply.started":"2021-08-26T23:44:22.506389Z","shell.execute_reply":"2021-08-26T23:44:22.515269Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# U-Net Implementation","metadata":{}},{"cell_type":"code","source":"def double_convolution(in_channels,out_channels):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channels,out_channels,kernel_size=3),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3),\n        nn.ReLU(inplace=True)\n    )\n    return conv\n\n\ndef crop_image_tensor(original_tensor,target_tensor):\n    \"\"\"\n    format of tensor in pytorch batch_size,channels,Height,Width\n    original_tensor:tensor to crop\n    target_tensor:target tensor which shoukd be smaller than original tensor\n    \"\"\"\n    original_tensor_size = original_tensor.size()[-1]\n    target_tensor_size = target_tensor.size()[-1]\n    #change = math.ceil((original_tensor_size - target_tensor_size)/2)\n    change = original_tensor_size - target_tensor_size\n    left=change//2\n    right=change-left\n    #print(\"x {} y {} change {}\".format(original_tensor_size, target_tensor_size, change))\n    # change =change//2\n    # x = original_tensor.size()[-1]\n    # y = target_tensor.size()[-1]\n    # change = max(math.floor((x-y)/2), ((x-y)//2))\n    \n    new = original_tensor[:,:,left:original_tensor_size - right ,left:original_tensor_size - right]\n    \n    return new\n    \n\n\n\nclass UNet(nn.Module):\n\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.max_pool_2_x_2 = nn.MaxPool2d(stride=2,kernel_size=2)\n\n        self.down_conv_block_1 = double_convolution(3,64)\n        self.down_conv_block_2 = double_convolution(64,128)\n        self.down_conv_block_3 = double_convolution(128,256)\n        self.down_conv_block_4 = double_convolution(256,512)\n        self.down_conv_block_5 = double_convolution(512,1024)\n\n        self.up_transpose_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2,stride=2)\n        self.up_conv_block_1 = double_convolution (1024,512)\n\n        self.up_transpose_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2,stride=2)\n        self.up_conv_block_2 = double_convolution (512,256)\n\n        self.up_transpose_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2,stride=2)\n        self.up_conv_block_3 = double_convolution (256,128)\n\n        self.up_transpose_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2,stride=2)\n        self.up_conv_block_4 = double_convolution (128,64)\n\n        self.output =nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)\n\n    def forward(self,image):\n        \"\"\"\n        bs,C,H,W\n        Contacting Path\n\n        \"\"\"\n\n        x1 =self. down_conv_block_1(image) ##concat1\n        # print(x1.size())\n        x2 = self.max_pool_2_x_2(x1)\n        x3 = self. down_conv_block_2(x2)##concat2\n        x4 = self.max_pool_2_x_2(x3)\n        x5 = self. down_conv_block_3(x4)##concat3\n        x6 = self.max_pool_2_x_2(x5)\n        x7 = self. down_conv_block_4(x6)##concat4\n        x8 = self.max_pool_2_x_2(x7)\n        x9 = self. down_conv_block_5(x8)\n        # print(x9.size())\n\n        x = self.up_transpose_1(x9)\n        y = crop_image_tensor(x7,x)\n        print(x.size())\n        print(y.size())\n        x = self.up_conv_block_1(torch.cat([x,y],axis=1))\n\n        x = self.up_transpose_2(x)\n        y = crop_image_tensor(x5,x)\n        print(x.size())\n        print(y.size())\n        x = self.up_conv_block_2(torch.cat([x,y],axis=1))\n\n        x = self.up_transpose_3(x)\n        y = crop_image_tensor(x3,x)\n        x = self.up_conv_block_3(torch.cat([x,y],axis=1))\n\n        x = self.up_transpose_4(x)\n        y = crop_image_tensor(x1,x)\n        x = self.up_conv_block_4(torch.cat([x,y],axis=1))\n\n        #print(\"size of cropped x7\",x.size())\n\n        output =self.output(x)\n        # print(output)\n        # print(output.size())\n        return output\n\n# if __name__== \"__main__\":\n#     image =torch.rand((1,3,572,572))\n#     model = UNet()\n#     print(model(image).size())\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T00:37:48.444045Z","iopub.execute_input":"2021-08-27T00:37:48.444401Z","iopub.status.idle":"2021-08-27T00:37:48.466651Z","shell.execute_reply.started":"2021-08-27T00:37:48.444367Z","shell.execute_reply":"2021-08-27T00:37:48.465531Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\nDATA_DIR = '../input/siim-png-images/train_png'\ndata_csv = '../input/siim-acr-pneumothorax-segmentation-data/train-rle.csv'\nbatch_size = 16\n","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.809301Z","iopub.execute_input":"2021-08-26T23:44:22.809873Z","iopub.status.idle":"2021-08-26T23:44:22.818457Z","shell.execute_reply.started":"2021-08-26T23:44:22.809831Z","shell.execute_reply":"2021-08-26T23:44:22.817678Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\ntransform = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5),\n    A.OneOf([A.RandomGamma(gamma_limit=(90,110)),\n             A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)], p=0.5),\n    A.Resize(width=224, height=224)\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.820077Z","iopub.execute_input":"2021-08-26T23:44:22.820823Z","iopub.status.idle":"2021-08-26T23:44:22.828146Z","shell.execute_reply.started":"2021-08-26T23:44:22.820779Z","shell.execute_reply":"2021-08-26T23:44:22.827358Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(data_csv)\ndf_train, df_val = model_selection.train_test_split(df, test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:22.830355Z","iopub.execute_input":"2021-08-26T23:44:22.830654Z","iopub.status.idle":"2021-08-26T23:44:23.015417Z","shell.execute_reply.started":"2021-08-26T23:44:22.830628Z","shell.execute_reply":"2021-08-26T23:44:23.014575Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = SIIMDataset(df_train,\n                            DATA_DIR,\n                            transform = transform,\n                            preprocessing_fun = preprocess_input)\n\nval_dataset = SIIMDataset(df_val,\n                          DATA_DIR,\n                          transform = transform,\n                          preprocessing_fun = preprocess_input)\n\ntrain_loader = DataLoader(train_dataset,\n                          batch_size = batch_size,\n                          shuffle = True,\n                          num_workers = 8)\n\nval_loader = DataLoader(val_dataset,\n                        batch_size = batch_size,\n                        num_workers = 4)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:23.017244Z","iopub.execute_input":"2021-08-26T23:44:23.017645Z","iopub.status.idle":"2021-08-26T23:44:23.026048Z","shell.execute_reply.started":"2021-08-26T23:44:23.017605Z","shell.execute_reply":"2021-08-26T23:44:23.024863Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print('Training data Info:')\ndataiter = iter(train_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))\n\nprint('\\nValidation data Info:')\ndataiter = iter(val_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:23.027686Z","iopub.execute_input":"2021-08-26T23:44:23.028491Z","iopub.status.idle":"2021-08-26T23:44:35.908291Z","shell.execute_reply.started":"2021-08-26T23:44:23.028448Z","shell.execute_reply":"2021-08-26T23:44:35.907019Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training data Info:\nshape of images : torch.Size([16, 3, 224, 224])\nshape of labels : torch.Size([16, 1, 224, 224])\n\nValidation data Info:\nshape of images : torch.Size([16, 3, 224, 224])\nshape of labels : torch.Size([16, 1, 224, 224])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Create Loss Class Defined By Kaggle**\n\n* Here, Dice Loss is define as well as Focal Loss is also created to get better results.\n* For this notebook, I have used the combination of two loss: diceloss and focalloss","metadata":{}},{"cell_type":"code","source":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=None):\n        super().__init__()\n\n    def forward(self, input, target):\n        loss = -torch.log(dice_loss(input, target))\n\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:44:48.341240Z","iopub.execute_input":"2021-08-26T23:44:48.341798Z","iopub.status.idle":"2021-08-26T23:44:48.354943Z","shell.execute_reply.started":"2021-08-26T23:44:48.341757Z","shell.execute_reply":"2021-08-26T23:44:48.353916Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **Train The Model**","metadata":{}},{"cell_type":"code","source":"# Create Model\nmodel = UNet()\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T00:38:05.398409Z","iopub.execute_input":"2021-08-27T00:38:05.398748Z","iopub.status.idle":"2021-08-27T00:38:05.696510Z","shell.execute_reply.started":"2021-08-27T00:38:05.398717Z","shell.execute_reply":"2021-08-27T00:38:05.695546Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"UNet(\n  (max_pool_2_x_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (down_conv_block_1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (down_conv_block_2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (down_conv_block_3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (down_conv_block_4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (down_conv_block_5): Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (up_transpose_1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n  (up_conv_block_1): Sequential(\n    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (up_transpose_2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n  (up_conv_block_2): Sequential(\n    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (up_transpose_3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n  (up_conv_block_3): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (up_transpose_4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (up_conv_block_4): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (output): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 3\ntrain_loss_ = []\nval_loss_ = []\n\nfor epoch in range(epochs):\n\n    train_loss = train(train_loader,\n                       model,\n                       criterion,\n                       optimizer)\n    train_loss_.append(train_loss)\n\n    val_loss = evaluate(val_loader,\n                        model,\n                        criterion)\n    val_loss_.append(val_loss)\n\n\n    print(f'Epoch: {epoch+1}')\n    print(f'Train Loss: {train_loss}, \\t Valid Loss: {val_loss}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T00:01:54.727765Z","iopub.execute_input":"2021-08-27T00:01:54.728118Z","iopub.status.idle":"2021-08-27T00:20:50.138005Z","shell.execute_reply.started":"2021-08-27T00:01:54.728086Z","shell.execute_reply":"2021-08-27T00:20:50.136751Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|██████████| 616/616 [05:25<00:00,  1.89it/s]\n100%|██████████| 109/109 [00:52<00:00,  2.09it/s]\n  0%|          | 0/616 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1\nTrain Loss: 4.981844440683142, \t Valid Loss: 5.297985829344583\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 616/616 [05:27<00:00,  1.88it/s]\n100%|██████████| 109/109 [00:52<00:00,  2.08it/s]\n  0%|          | 0/616 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2\nTrain Loss: 5.023431844525523, \t Valid Loss: 5.303195629644831\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 616/616 [05:26<00:00,  1.89it/s]\n100%|██████████| 109/109 [00:51<00:00,  2.12it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3\nTrain Loss: 4.981472322693119, \t Valid Loss: 5.305510717794435\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train and Val Loss Curves","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(train_loss_,'-o')\nplt.plot(val_loss_,'-o')\nplt.xlabel('epoch')\nplt.ylabel('Losses')\nplt.legend(['Train','Valid'])\nplt.title('Train vs Valid Loss')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T00:23:30.728131Z","iopub.execute_input":"2021-08-27T00:23:30.728523Z","iopub.status.idle":"2021-08-27T00:23:30.968770Z","shell.execute_reply.started":"2021-08-27T00:23:30.728489Z","shell.execute_reply":"2021-08-27T00:23:30.967744Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}